{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Tue Dec  3 12:10:13 2019\\n\\n@author: SWH\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Dec  3 12:10:13 2019\n",
    "\n",
    "@author: SWH\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow 경고 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로우 경고 무시하는 코드\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "try:\n",
    "    from tensorflow.python.util import module_wrapper as deprecation\n",
    "except ImportError:\n",
    "    from tensorflow.python.util import deprecation_wrapper as deprecation\n",
    "deprecation._PER_MODULE_WARNING_LIMIT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 임포트\n",
    "import os,random\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "import keras.models as models\n",
    "from keras.layers.core import Reshape,Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import adam\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cmath\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pkl 파일 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with open('D:\\OneDrive\\RML2016.10a_dict.pkl', 'rb') as f:\n",
    "    u = pickle._Unpickler(f)\n",
    "    u.encoding = 'latin1'\n",
    "    Xd = u.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<choice the vectors. a: IQ, b: phase_amp, c: FFT >b\n"
     ]
    }
   ],
   "source": [
    "data = input(\"<choice the vectors. a: IQ, b: phase_amp, c: FFT >\") # 분류할 벡터 선택, 여기서는 b 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs, mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1,0])\n",
    "\n",
    "X = []  \n",
    "lbl = []\n",
    "for mod in mods:\n",
    "    for snr in snrs:\n",
    "        X.append(Xd[(mod,snr)])\n",
    "        for i in range(Xd[(mod,snr)].shape[0]):  lbl.append((mod,snr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector 데이터 저장 및 pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = np.vstack(X) # iq 벡터 데이터 저장 \n",
    "\n",
    "x_all = x_all.swapaxes(2, 1) # cnn의 input으로 넣기 위해 축 변환\n",
    "\n",
    "x_all = np.array(x_all)\n",
    "\n",
    "x_r = x_all[:,:,0] # real 분리\n",
    "x_i = x_all[:,:,1] # imagine 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_r_flat = np.ravel(x_r, order='c') # phase vector를 계산하기 위해 1차원으로 폄\n",
    "x_i_flat = np.ravel(x_i, order='c') # phase vector를 계산하기 위해 1차원으로 폄\n",
    "x_phase = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase/Amp/FFT vector 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "waiting...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"waiting...\")\n",
    "for i in range(len(x_i_flat)):\n",
    "    x_phase.append(cmath.atan(x_r_flat[i]/x_i_flat[i])) # phase vector 계산\n",
    "\n",
    "print(\"done.\")    \n",
    "x_phase = np.array(x_phase)\n",
    "\n",
    "x_phase = x_phase.reshape(220000,128) \n",
    "x_amp = (x_r**2+x_i**2)**1/2 # amplitude vector 계산\n",
    "\n",
    "x_fft_r = np.fft.fft(x_r) # fft vector 계산\n",
    "x_fft_i = np.fft.fft(x_i) # fft vector 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_phase_amp = np.dstack((x_phase, x_amp)) # phase와 amplitude vector 합침\n",
    "x_fft = np.dstack((x_fft_r, x_fft_i)) # FFT vector 합침"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 및 test set 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777) # 랜덤으로 train을 뽑을 거기때문에 랜덤 시드 고정\n",
    "n_examples = x_all.shape[0] # 총 데이터 갯수\n",
    "n_train = n_examples * 0.67 # 67%만큼 train에 사용\n",
    "train_idx = np.random.choice(220000, size=int(n_train), replace=False) # 전체에서 67%만큼 train index 뽑음\n",
    "test_idx = list(set(range(0,n_examples))-set(train_idx)) # 전체에서 train 뽑은 나머지를 test set으로 (33%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase and amp vector selected.\n"
     ]
    }
   ],
   "source": [
    "# a b c 선택시 분류에 사용할 vector가 달라짐\n",
    "if data == 'a': \n",
    "    x_train = x_all[train_idx]\n",
    "    x_test =  x_all[test_idx]\n",
    "    print(\"IQ vector selected.\")\n",
    "if data == 'b':\n",
    "    x_train = x_phase_amp[train_idx]\n",
    "    x_test =  x_phase_amp[test_idx]\n",
    "    print(\"phase and amp vector selected.\")\n",
    "if data == 'c':\n",
    "    x_train = x_fft[train_idx]\n",
    "    x_test =  x_fft[test_idx]\n",
    "    print(\"FFT vector selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(list(map(lambda x: mods.index(lbl[x][0]), train_idx)))\n",
    "y_test = to_categorical(list(map(lambda x: mods.index(lbl[x][0]), test_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall, Precision, f1 score 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, f1 score 함수\n",
    "\n",
    "def recall_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_metric(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    precision = precision_metric(y_true, y_pred)\n",
    "    recall = recall_metric(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특정 SNR일때 평가를 계산하기 위한 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 특정 snr일때 (18, 0, -8) 평가를 확인하기 위한 setting\n",
    "\n",
    "test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "test_X_i = x_test[np.where(np.array(test_SNRs)==18)] #==18이 default.\n",
    "test_Y_i = y_test[np.where(np.array(test_SNRs)==18)] #==18이 default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network starting...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 126, 256)          1792      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 126, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 80)           61520     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 124, 80)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9920)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2539776   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                2827      \n",
      "=================================================================\n",
      "Total params: 2,605,915\n",
      "Trainable params: 2,605,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 147400 samples, validate on 72600 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\3ttook\\Anaconda3\\envs\\kera\\lib\\site-packages\\numpy\\core\\_asarray.py:85: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147400/147400 [==============================] - 13s 86us/step - loss: 2.1529 - acc: 0.2106 - precision_metric: 0.3679 - recall_metric: 0.0261 - f1_metric: 0.0477 - val_loss: 1.8766 - val_acc: 0.3044 - val_precision_metric: 0.2981 - val_recall_metric: 0.1053 - val_f1_metric: 0.1454\n",
      "Epoch 2/10\n",
      "147400/147400 [==============================] - 10s 66us/step - loss: 1.8848 - acc: 0.2902 - precision_metric: 0.6260 - recall_metric: 0.0998 - f1_metric: 0.1719 - val_loss: 1.7691 - val_acc: 0.3335 - val_precision_metric: 0.3133 - val_recall_metric: 0.1345 - val_f1_metric: 0.1795\n",
      "Epoch 3/10\n",
      "147400/147400 [==============================] - 10s 66us/step - loss: 1.8001 - acc: 0.3177 - precision_metric: 0.6704 - recall_metric: 0.1264 - f1_metric: 0.2124 - val_loss: 1.7050 - val_acc: 0.3521 - val_precision_metric: 0.3746 - val_recall_metric: 0.1541 - val_f1_metric: 0.1991\n",
      "Epoch 4/10\n",
      "147400/147400 [==============================] - 10s 66us/step - loss: 1.7488 - acc: 0.3345 - precision_metric: 0.6921 - recall_metric: 0.1454 - f1_metric: 0.2401 - val_loss: 1.6532 - val_acc: 0.3733 - val_precision_metric: 0.4356 - val_recall_metric: 0.1793 - val_f1_metric: 0.2313\n",
      "Epoch 5/10\n",
      "147400/147400 [==============================] - 10s 66us/step - loss: 1.7141 - acc: 0.3468 - precision_metric: 0.7105 - recall_metric: 0.1590 - f1_metric: 0.2596 - val_loss: 1.6401 - val_acc: 0.3796 - val_precision_metric: 0.4170 - val_recall_metric: 0.1811 - val_f1_metric: 0.2304\n",
      "Epoch 6/10\n",
      "147400/147400 [==============================] - 10s 66us/step - loss: 1.6926 - acc: 0.3556 - precision_metric: 0.7224 - recall_metric: 0.1697 - f1_metric: 0.2748 - val_loss: 1.6141 - val_acc: 0.3856 - val_precision_metric: 0.4358 - val_recall_metric: 0.1993 - val_f1_metric: 0.2555\n",
      "Epoch 7/10\n",
      "147400/147400 [==============================] - 10s 67us/step - loss: 1.6747 - acc: 0.3607 - precision_metric: 0.7291 - recall_metric: 0.1774 - f1_metric: 0.2853 - val_loss: 1.6045 - val_acc: 0.3908 - val_precision_metric: 0.4537 - val_recall_metric: 0.2020 - val_f1_metric: 0.2580\n",
      "Epoch 8/10\n",
      " 38912/147400 [======>.......................] - ETA: 6s - loss: 1.6697 - acc: 0.3656 - precision_metric: 0.7325 - recall_metric: 0.1794 - f1_metric: 0.2881"
     ]
    }
   ],
   "source": [
    "# network 설정\n",
    "print(\"Network starting...\")\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu', input_shape=(128,2)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Conv1D(filters=80, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(11, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\", precision_metric, recall_metric, f1_metric])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=1024, epochs=10, verbose=1) # 여기서는 10 epoch 만\n",
    "\n",
    "loss, accuracy, precision, recall, f1 = model.evaluate(test_X_i, test_Y_i, verbose=1, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, precision, recall, f1 score 값 출력\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusision matrix 함수,아래 링크 참고함\n",
    "\n",
    "#https://github.com/radioML/examples/blob/master/modulation_recognition/RML2016.10a_VTCNN2_example.ipynb\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snr -20부터 18까지의 정확도 출력,아래 링크 참고함\n",
    " #https://github.com/radioML/examples/blob/master/modulation_recognition/RML2016.10a_VTCNN2_example.ipynb\n",
    "\n",
    "\n",
    "acc = {}\n",
    "for snr in snrs:\n",
    "\n",
    "    # extract classes @ SNR\n",
    "    test_SNRs = list(map(lambda x: lbl[x][1], test_idx))\n",
    "    test_X_i = x_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    test_Y_i = y_test[np.where(np.array(test_SNRs)==snr)]\n",
    "    \n",
    "    test_Y_i_hat = model.predict(test_X_i)\n",
    "    conf = np.zeros([11,11])\n",
    "    confnorm = np.zeros([11,11])\n",
    "    for i in range(0,test_X_i.shape[0]):\n",
    "        j = list(test_Y_i[i,:]).index(1)\n",
    "        k = int(np.argmax(test_Y_i_hat[i,:]))\n",
    "        conf[j,k] = conf[j,k] + 1\n",
    "    for i in range(0,11):\n",
    "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confnorm, labels=mods, title=\"ConvNet Confusion Matrix (SNR=%d)\"%(snr))\n",
    "    \n",
    "    cor = np.sum(np.diag(conf))\n",
    "    ncor = np.sum(conf) - cor\n",
    "    print(\"Accuracy:\", cor / (cor+ncor))\n",
    "    acc[snr] = 1.0*cor/(cor+ncor)\n",
    "\n",
    " # 해당 선택한 vector의 snr 종합 정확도 출력\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(snrs, list(map(lambda x: acc[x], snrs)))\n",
    "plt.xlabel(\"Signal to Noise Ratio\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Accuracy in SNR\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
